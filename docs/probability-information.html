<!doctype html>
<html lang="en">
    <head>
    

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-166292985-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-166292985-1');
    </script>
    

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" 
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons"
          rel="stylesheet" crossorigin="anonymous">
    
    <link rel="stylesheet" href="/theme/pygment-github.css">
    <link rel="stylesheet" href="/theme/article.css"> 

    <title>An information theory perspective on probability</title>
    
    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="120x120" href="/images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png">
    <link rel="manifest" href="/images/favicon/site.webmanifest">
    <link rel="mask-icon" href="/images/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/images/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    
<!-- Mathjax -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_SVG">
  MathJax.Hub.Config({
    tex2jax: {
    	inlineMath: [ ['$','$'] ],
    	processEscapes: true
    },
	"HTML-CSS": {
		fonts: ["TeX"] 
	}
  });
</script>

    </head>

    <body>
    
    <nav class="navbar navbar-expand-lg navbar-dark" style="background-color: rgb(78, 76, 103);">
        <a class="navbar-brand" href="//julienharbulot.com/index.html"><img src="//julienharbulot.com/images/logo.svg" width="30" alt=""></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
                    <a class="nav-link" href="//julienharbulot.com/technical-blog.html">Technical blog</a>
            </li>
            
            <li class="nav-item">
                    <a class="nav-link" href="//julienharbulot.com/maths.html">Maths</a>
            </li>
            
            <li class="nav-item">
                    <a class="nav-link" href="//julienharbulot.com/research.html">Research</a>
            </li>
            
            <li class="nav-item">
                    <a class="nav-link" href="//julienharbulot.com/projects.html">Projects</a>
            </li>
            
            <!--<li class="nav-item">
                <a class="nav-link" href="//julienharbulot.com/about-me.html">About me</a>
            </li>-->
            

<li class="nav-item dropdown">
    <a class="nav-link active dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
        Table of content
    </a>
    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
        <div class="toc"><ul>
<li class="toc-entry-1">
<a class='toc-href dropdown-item' href='#scale' title='Scale'>Scale</a>
</li>
<div class="dropdown-divider"></div>
<li class="toc-entry-1">
<a class='toc-href dropdown-item' href='#read-next' title='Read next'>Read next</a>
</li>
<div class="dropdown-divider"></div>
<li class="toc-entry-1">
<a class='toc-href dropdown-item' href='#reference' title='Reference'>Reference</a>
</li>
</ul><div>
    </div>
</li>


            </ul>
        </div>
    </nav>
    
    
    
    <div class="container">
    <div class="row mt-5">
      <div class="col-lg-8 mx-auto"> 
        <h1 class="display-5">An information theory perspective on probability </h1>
        <div class="text-right">11 Mar 2018</div>
      </div>
    </div>
    <div class="row mt-5">
    <article class="col-lg-8 mx-auto">
    <p>
<script type="math/tex; mode=display">
\def\sa{a}
\def\sb{b}
\def\sc{c}
\def\sd{d}
\def\se{e}
\def\sf{f}
\def\sg{g}
\def\sh{h}
\def\si{i}
\def\sj{j}
\def\sk{k}
\def\sl{l}
\def\sm{m}
\def\sn{n}
\def\so{o}
\def\sp{p}
\def\sq{q}
\def\sr{r}
\def\ss{s}
\def\st{t}
\def\su{u}
\def\sv{v}
\def\sw{w}
\def\sx{x}
\def\sy{y}
\def\sz{z}
\def\va{\vec{a}}
\def\vb{\vec{b}}
\def\vc{\vec{c}}
\def\vd{\vec{d}}
\def\ve{\vec{e}}
\def\vf{\vec{f}}
\def\vg{\vec{g}}
\def\vh{\vec{h}}
\def\vi{\vec{i}}
\def\vj{\vec{j}}
\def\vk{\vec{k}}
\def\vl{\vec{l}}
\def\vm{\vec{m}}
\def\vn{\vec{n}}
\def\vo{\vec{o}}
\def\vp{\vec{p}}
\def\vq{\vec{q}}
\def\vr{\vec{r}}
\def\vs{\vec{s}}
\def\vt{\vec{t}}
\def\vu{\vec{u}}
\def\vv{\vec{v}}
\def\vw{\vec{w}}
\def\vx{\vec{x}}
\def\vy{\vec{y}}
\def\vz{\vec{z}}
\def\ga{\mathfrak{A}}
\def\gb{\mathfrak{B}}
\def\gc{\mathfrak{C}}
\def\gd{\mathfrak{D}}
\def\ge{\mathfrak{E}}
\def\gf{\mathfrak{F}}
\def\gg{\mathfrak{G}}
\def\gh{\mathfrak{H}}
\def\gi{\mathfrak{I}}
\def\gj{\mathfrak{J}}
\def\gk{\mathfrak{K}}
\def\gl{\mathfrak{L}}
\def\gm{\mathfrak{M}}
\def\gn{\mathfrak{N}}
\def\go{\mathfrak{O}}
\def\gp{\mathfrak{P}}
\def\gq{\mathfrak{Q}}
\def\gr{\mathfrak{R}}
\def\gs{\mathfrak{S}}
\def\gt{\mathfrak{T}}
\def\gu{\mathfrak{U}}
\def\gv{\mathfrak{V}}
\def\gw{\mathfrak{W}}
\def\gx{\mathfrak{X}}
\def\gy{\mathfrak{Y}}
\def\gz{\mathfrak{Z}}
\def\ra{A}
\def\rb{B}
\def\rc{C}
\def\rd{D}
\def\re{E}
\def\rf{F}
\def\rg{G}
\def\rh{H}
\def\ri{I}
\def\rj{J}
\def\rk{K}
\def\rl{L}
\def\rm{M}
\def\rn{N}
\def\ro{O}
\def\rp{P}
\def\rq{Q}
\def\rr{R}
\def\rs{S}
\def\rt{T}
\def\ru{U}
\def\rv{V}
\def\rw{W}
\def\rx{X}
\def\ry{Y}
\def\rz{Z}
\def\rva{\vec{A}}
\def\rvb{\vec{B}}
\def\rvc{\vec{C}}
\def\rvd{\vec{D}}
\def\rve{\vec{E}}
\def\rvf{\vec{F}}
\def\rvg{\vec{G}}
\def\rvh{\vec{H}}
\def\rvi{\vec{I}}
\def\rvj{\vec{J}}
\def\rvk{\vec{K}}
\def\rvl{\vec{L}}
\def\rvm{\vec{M}}
\def\rvn{\vec{N}}
\def\rvo{\vec{O}}
\def\rvp{\vec{P}}
\def\rvq{\vec{Q}}
\def\rvr{\vec{R}}
\def\rvs{\vec{S}}
\def\rvt{\vec{T}}
\def\rvu{\vec{U}}
\def\rvv{\vec{V}}
\def\rvw{\vec{W}}
\def\rvx{\vec{X}}
\def\rvy{\vec{Y}}
\def\rvz{\vec{Z}}
\def\seta{A}
\def\setb{B}
\def\setc{C}
\def\setd{D}
\def\sete{E}
\def\setf{F}
\def\setg{G}
\def\seth{H}
\def\seti{I}
\def\setj{J}
\def\setk{K}
\def\setl{L}
\def\setm{M}
\def\setn{N}
\def\seto{O}
\def\setp{P}
\def\setq{Q}
\def\setr{R}
\def\sets{S}
\def\sett{T}
\def\setu{U}
\def\setv{V}
\def\setw{W}
\def\setx{X}
\def\sety{Y}
\def\setz{Z}
\def\fa{a}
\def\fb{b}
\def\fc{c}
\def\fd{d}
\def\fe{e}
\def\ff{f}
\def\fg{g}
\def\fh{h}
\def\fi{i}
\def\fj{j}
\def\fk{k}
\def\fl{l}
\def\fm{m}
\def\fn{n}
\def\fo{o}
\def\fp{p}
\def\fq{q}
\def\fr{r}
\def\fs{s}
\def\ft{t}
\def\fu{u}
\def\fv{v}
\def\fw{w}
\def\fx{x}
\def\fy{y}
\def\fz{z}
\def\fA{A}
\def\fB{B}
\def\fC{C}
\def\fD{D}
\def\fE{E}
\def\fF{F}
\def\fG{G}
\def\fH{H}
\def\fI{I}
\def\fJ{J}
\def\fK{K}
\def\fL{L}
\def\fM{M}
\def\fN{N}
\def\fO{O}
\def\fP{P}
\def\fQ{Q}
\def\fR{R}
\def\fS{S}
\def\fT{T}
\def\fU{U}
\def\fV{V}
\def\fW{W}
\def\fX{X}
\def\fY{Y}
\def\fZ{Z}
\def\ma{A}
\def\mb{B}
\def\mc{C}
\def\md{D}
\def\me{E}
\def\mf{F}
\def\mg{G}
\def\mh{H}
\def\mi{I}
\def\mj{J}
\def\mk{K}
\def\ml{L}
\def\mm{M}
\def\mn{N}
\def\mo{O}
\def\mp{P}
\def\mq{Q}
\def\mr{R}
\def\ms{S}
\def\mt{T}
\def\matu{U}
\def\mv{V}
\def\mw{W}
\def\mx{X}
\def\my{Y}
\def\mz{Z}
\def\loss{\mathcal{L}}
\newcommand{\dkl}[2]{D_{\text{KL}}\mathopen{}\paren{#1\,||\,#2}}
\newcommand{\dataset}{S}
\newcommand{\ndataset}{N}
\newcommand{\idataset}{n}
\newcommand{\inputRV}{\mathcal{X}}
\newcommand{\inputvec}{\vec{x}}
\newcommand{\ninputvec}[1]{\vec{x}_{#1}}
\newcommand{\iinputvec}[1]{x_{#1}}
\newcommand{\niinputvec}[2]{x_{#1, #2}}
\newcommand{\icpnt}{i}
\newcommand{\inputmatrix}{X}
\newcommand{\inputdim}{D}
\newcommand{\outputval}{y}
\newcommand{\ioutputval}[1]{y_{#1}}
\newcommand{\outputvec}{\vec{y}}
\newcommand{\trainset}{S_{\text{train}}}
\newcommand{\testset}{S_{\text{test}}}
\newcommand{\truemodel}{f_{\text{true}}}
\newcommand{\trainedmodel}{f_{\trainset}}
\newcommand{\linmodel}[1]{f_{#1}}
\newcommand{\bestmodel}{f^{*}}
\newcommand{\model}{f}
\newcommand{\hyperparam}{\lambda}
\newcommand{\linparamv}{\vec{w}}
\newcommand{\ilinparam}[1]{w_{#1}}
\newcommand{\indivloss}{l}
\newcommand{\modelclass}{\mathcal{F}}
\newcommand{\linclass}{\modelclass_{\text{lin}}}
\newcommand{\g}{\mathcal{G}}
\newcommand{\gmse}{\g_{\text{MSE}}}
\newcommand{\glasso}{\g_{\text{lasso}}}
\newcommand{\gridge}{\g_{\text{ridge}}}
\newcommand{\glogit}{\g_{\logit}}
\newcommand{\l}{\mathcal{L}}
\newcommand{\lmse}{\l_{\text{MSE}}}
\newcommand{\lmae}{\l_{\text{MAE}}}
\newcommand{\llasso}{\l_{\text{lasso}}}
\newcommand{\lridge}{\l_{\text{ridge}}}
\newcommand{\llogit}{\l_{\logit}}
\newcommand{\logit}{\sigma}
\newcommand{\reg}{\mathcal{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\mean}{mean}
\DeclareMathOperator*{\avg}{avg}
\DeclareMathOperator*{\span}{span}
\DeclareMathOperator*{\var}{var}
\DeclareMathOperator*{\bias}{bias}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\brak}[1]{\left[#1\right]}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\realset}{\mathbb{R}}
\newcommand{\realvset}[1]{\realset^{#1}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\gaussian}{\mathcal{N}}
\newcommand{\iid}{\stackrel{\text{i.i.d.}}{\sim}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normtwo}[1]{\norm{#1}_{2}}
\newcommand{\normone}[1]{\norm{#1}_{1}}
\newcommand{\card}[1]{\left\lvert#1\right\rvert}
\newcommand{\grad}{\nabla}
\newcommand{\dconv}{\stackrel{d}{\to}}
\newcommand{\pconv}{\stackrel{p}{\to}}
\newcommand{\rva}[1]{#1}
\newcommand{\rve}[1]{\vec{#1}}
\newcommand{\obs}[1]{#1}
\newcommand{\vobs}[1]{\vec{#1}}
\newcommand{\distrib}[1]{#1}
\newcommand{\distribof}[2]{#1_{#2}}
\newcommand{\density}[1]{#1}
\newcommand{\densityof}[2]{#1_{#2}}
\newcommand{\distributed}{\sim}
\newcommand{\const}[1]{#1}
\newcommand{\fun}[1]{#1}
</script>
</p>
<div class="typography">

<p>In 1948, Claude Shannon invented information theory based on probability theory. The basic definition is entropy. Given of a set of messages mi, each one occurring with probability pi, their entropy is defined as <script type="math/tex">–\sum_i p_i × log(p_i)</script> where <script type="math/tex">log</script> is logarithm base 2 . The messages could be letters in an alphabet, or words in a language, and the idea is that a long sequence of messages is sent from a sender to a receiver. The probability pi is the relative frequency of message mi in the sequence. Shannon referred to entropy as a measure of “uncertainty” on the part of the receiver, before receiving a message, about what message would be received next. It is independent of representation.</p>
<p>The word “entropy” comes from statistical mechanics, where it originally represented the amount of “disorder” in a large collection of molecules. Currently it is explained as the average energy carried by a molecule, which is related by the Boltzmann constant <script type="math/tex">k ≈ 1.38×10^{–23}</script> to the temperature. Although temperature is considered a macro property, and one may be reluctant to talk about the average value in a set that contains only one value, there is no harm in relating energy to temperature even for a single molecule.</p>
<script type="math/tex; mode=display">E = k×T/2</script>
<p>Similarly, Shannon was reluctant to talk about the information content of each message individually, but there is no harm in doing so. If we define the information content <script type="math/tex">I_i</script> of message <script type="math/tex">m_i</script> as:</p>
<script type="math/tex; mode=display">I_i = – \log(p_i)</script>
<p>then, the entropy:</p>
<script type="math/tex; mode=display">\sum_i p_i × I_i</script>
<p>is the average information content of a message measured in bits.</p>
<p>In 1948 it made good sense to explain information in terms of probability; information (as a mathematical theory) was unknown, and probability (as a mathematical theory) was already well developed. But today it might make better sense to explain probability in terms of information. Most people today have a quantitative idea of what information and memory are; they talk about bits and bytes; they buy an amount of memory, and hold it in their hand; they wait for a download, and complain about the bandwidth. Many people already understand the important difference between information and memory; they compress files before sending them, and they decompress files upon receiving them.</p>
<p>Information theory talks about messages, but it could just as well talk about events, or outcomes of an experiment. (Perhaps a message is just a special case of event, or perhaps an event is just a special case of message.) Let us be more abstract, and dispense with events and messages. The information <script type="math/tex">I</script> (in bits) associated with probability <script type="math/tex">p</script> is:</p>
<script type="math/tex; mode=display">I = –\log p</script>
<p>which is easily inverted:</p>
<script type="math/tex; mode=display">p = 2^{–I}</script>
<p>to allow us to define probability in terms of information. The suggestion to define probability in terms of information is intended as a pedagogical technique: define the less familiar in terms of the more familiar, or perhaps I mean define the less understood in terms of the more understood. Henceforth I will be neutral on this point, making use of the relationship between them, without taking either one of them to be more basic.</p>
<p>Shannon explained the amount of information carried by a message as a measure of how surprised one is to learn the message. Probability is also a measure of surprise, or inversely, expectation. If there are two possibilities A and B, and I say it will probably be A, I mean that I expect to see A, and I will be surprised to see B . A numeric probability expresses the strength of my expectation to see A, and the amount of my surprise if I see B . One’s expectation and surprise may be shaped by past frequencies, or they could be shaped by considerations that apply to one-time-only events.</p>
<h2 id="scale">Scale</h2>
<p>There are two temperature scales in common use: Fahrenheit (in the USA) and Celsius (in the rest of the world). There are formulas to convert each to the other:</p>
<table>
<tbody>
<tr>
<td><script type="math/tex">c = (f–32)×\frac{5}{9}</script></td>
<td>and</td>
<td><script type="math/tex">f = c×\frac{9}{5} + 32</script></td>
</tr>
</tbody>
</table>
<p>Whenever two physical quantities can be converted, each to the other, they measure the same thing on different scales. (More generally, every physical law says that there are fewer things to measure than there are variables in the law.) So energy and mass measure the same thing on different scales: <script type="math/tex">E=m×c^2</script> and <script type="math/tex">m=E/c^2</script>.</p>
<p>More to the point, information and probability measure the same thing on different scales.</p>
<table>
<tbody>
<tr>
<td><script type="math/tex">I=–\log p</script></td>
<td>and</td>
<td><script type="math/tex">p=2^{–I}</script></td>
</tr>
</tbody>
</table>
<p>I am not sure what to call the “thing” measured on these two scales; rather than introduce a new word I shall just call it “information”.</p>
<p>There is another scale in common use for measuring information: the number of possible states. (This same scale applies to energy-temperature-mass too.) This is the scale preferred by people who build “model checkers” to verify the correctness of computer hardware or software. They like to say they can handle up to <script type="math/tex">10^{60}</script> states, which is something like the number of atoms in our galaxy. That is a truly impressive number, until we realize that <script type="math/tex">10^{60}</script> is about <script type="math/tex">2^{200}</script>, which is the state space of 200 bits, or about six 32-bit variables; we rapidly descend from <script type="math/tex">10^{60}</script> states to 6 program variables!</p>
<p>In order to write the conversion formulas among the three scales neatly, I need unit names for each of them. We already have the “bit” and the “state”; I am missing a unit for the probability scale, so let me invent the “chance”. (All three of these units are non- physical; they are alternative names for unity (pure numbers).) Here are the conversions.</p>
<table>
<tbody>
<tr>
<td><script type="math/tex">b</script> bit</td>
<td>= <script type="math/tex">2^b</script> state</td>
<td>= <script type="math/tex">2^{–b}</script> chance</td>
</tr>
<tr>
<td><script type="math/tex">s</script> state</td>
<td>= <script type="math/tex">1/s</script> chance</td>
<td>= <script type="math/tex">\log s</script> bit</td>
</tr>
<tr>
<td><script type="math/tex">c</script> chance</td>
<td>= <script type="math/tex">-\log c</script> bit</td>
<td>= <script type="math/tex">1/c</script> state</td>
</tr>
</tbody>
</table>
<p>Let’s look at three example point on these scales.</p>
<table>
<tbody>
<tr>
<td>0 bit</td>
<td>= 1 state</td>
<td>= 1 chance</td>
</tr>
<tr>
<td>1 bit</td>
<td>= 2 state</td>
<td>= 0.5 chance</td>
</tr>
<tr>
<td><script type="math/tex">\inf</script> bit</td>
<td>= <script type="math/tex">\inf</script> state</td>
<td>= 0 chance</td>
</tr>
</tbody>
</table>
<p>On the middle line, 1 bit is the amount of information needed to tell us which of 2 states we are in, or has occurred, or will occur, and that corresponds to probability 1/2 chance for each state. On the top line, 0 bits is the amount of information needed to tell us which state if there is only 1 state, and that corresponds to 1 chance (certainty). On the bottom line, it takes <script type="math/tex">\inf</script> bits to tell us that something impossible is occurring (Shannon would say that we are infinitely surprised). (I say “certain” for probability 1 and “impossible” for probability 0 and I don’t care about any measure-theoretic difference.) […]</p>
<p>The “problem of prior probabilities” is the problem of how Bayesians justify the assumption that the initial probability distribution is uniform across all states. I suggest that there is no “assumption” being made, and so no need for “justification”.</p>
<p>Saying that there are 4 states is saying, on another scale, that the probability is 1/4, and on yet another scale that 2 bits are required to specify the situation. If we then learn that one of the states never occurs, we adjust: there are 3 states (that occur); each of the (occurring) states has probability 1/3 (and any nonoccurring state has probability 0 ); it takes about 1.585 bits to identify a state (that occurs, and infinitely many bits to identify any nonoccurring state).</p>
<p>To be less extreme, if we learn that one of the four states rarely occurs, then we adjust: as a measure of information, there are less than 4 but more than 3 states ; each commonly occurring state has a probability between 1/4 and 1/3, and the rarely occurring state has a probability between 0 and 1/4 ; it takes somewhere between 1.585 and 2 bits to identify any of the commonly occurring states, and somewhere between 2 and ∞ bits to identify the rarely occurring state. In general, having no prior information about which of n states occurs is probability 1/n for each state, not by assumption, but by a change of scale. […]</p>
<p>What is the point of having several scales on which to measure the same quantity? If they are Fahrenheit and Celsius for measuring temperature, there is no point at all; they are linear translations of each other, and the duplication is just annoying. A slide rule multiplies two numbers by transforming them to a logarithmic scale, where the multiplication is transformed into the simpler operation of addition, and then transforms the result back. Fourier transforms are used for the same reason. Similarly, perhaps some information calculations are easier on the chance (probability) scale, others on the bit scale, and still others on the state scale. Thus they might all be useful.</p>
<h2 id="read-next">Read next</h2>
<p>See how to construct probability theory based on those insights in my next article: <a href="probability-logic-of-uncertainty.html">Probability theory, the logic of uncertainty</a></p>
<h2 id="reference">Reference</h2>
<p>This article is an excerpt from the 44-pages article “<em>a Probability Perspective</em>” published by <em>Eric C.R. Hehner</em>. You can find the full article <a href="http://www.cs.utoronto.ca/~hehner/proper.pdf">here</a>. In this article Hehner give the above perpective on Information theory, then the <a href="probability-bayesian-perspective.html">Bayesian perspective</a> and the modeling of real world events using a new probabilistic formalism based on programmation, that he calls the programmer’s perspective. His formalism allows to solve classical paradox in an easy and non-ambiguous way.</p>
</div>
    </article>
    </div><!-- row -->
    
    <div class="row mt-1">
      <article class="col-lg-8 mx-auto recommendations">
        <p class='title'>Other articles you might like:</p>
        <ul>
          
            <li class="en"><img class="flag-icon" src="//julienharbulot.com/images/languages/en.png"> <a href="//julienharbulot.com/ridge-regression.html">What is ridge regression?</a></li>
          
            <li class="en"><img class="flag-icon" src="//julienharbulot.com/images/languages/en.png"> <a href="//julienharbulot.com/linear-regression-explained.html">Linear regressions in simple terms</a></li>
          
            <li class="en"><img class="flag-icon" src="//julienharbulot.com/images/languages/en.png"> <a href="//julienharbulot.com/feature.html">What is a feature?</a></li>
          
            <li class="en"><img class="flag-icon" src="//julienharbulot.com/images/languages/en.png"> <a href="//julienharbulot.com/residuals.html">What is a residual?</a></li>
          
            <li class="en"><img class="flag-icon" src="//julienharbulot.com/images/languages/en.png"> <a href="//julienharbulot.com/mean-squared-error-loss.html">The MSE loss</a></li>
          
        </ul>
      </article>
    </div>
    
    
    <div class="row mb=5 mt-5">
      <article class="col-lg-8 mx-auto">
        <div id="hyvor-talk-view"></div>
        <script type="text/javascript">
            var HYVOR_TALK_WEBSITE = '625';
            var HYVOR_TALK_CONFIG = {
                url: 'probability-information.html',
                id: 'probability-information.html'
            };
        </script>
      </article>
    </div><!-- row -->
    
    <footer class="row">
        <div class="col-lg-8 mx-auto text-center">
          <p><small>
          
            Last updated: 01/14/21 <br/>
          
          
          Copyright &copy; 2021 Julien Harbulot
          </small></p>
        </div>
    </footer>
    </div>

  <script>
  // bootstrap table
  tables = document.getElementsByTagName('table');
  for (i = 0; i < tables.length; i++) {
    tables[i].classList.add('table');
    tables[i].classList.add('table-bordered');
  }

  // Paragraphs that contain only images are marked with custom class for styling
  ps = document.getElementsByTagName('p');
  Array.from(ps).forEach(function(p){
    if (p.getElementsByTagName('img').length > 0 && p.textContent.trim().length == 0) {
        p.classList.add('img-container');
    }
  });
  </script>


    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script>
        prev_onload_a = window.onload;
        window.onload = function() {
            if (prev_onload_a) {
                prev_onload_a();
            }
            var navlinks = document.getElementsByClassName("nav-link");
            var i;
            for (i = 0; i < navlinks.length; i++) {
                const target = new URL(navlinks[i].href)
                if (location.pathname == target.pathname) {
                    navlinks[i].classList.add('active')
                }
            }
        }
    </script>    

    

<script async type="text/javascript" src="//talk.hyvor.com/web-api/embed"></script>


    </body>
</html>